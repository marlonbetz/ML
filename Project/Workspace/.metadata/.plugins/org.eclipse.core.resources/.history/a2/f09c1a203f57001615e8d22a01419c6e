import numpy as np 
from keras.models import Model
from keras.layers import GRU,Dense,Input,Merge,Embedding
import regex
import codecs
from nltk import ngrams
def getListofASJPPhonemes(word):
    phonemes_alone="pbmfv84tdszcnSZCjT5kgxNqGX7hlLwyr!ieaouE3"
    phonemeSearchRegex = "["+phonemes_alone+"][\"\*]?(?!["+phonemes_alone+"]~|["+phonemes_alone+"]{2}\$)|["+phonemes_alone+"]{2}?~|["+phonemes_alone+"]{3}?\$"
    return regex.findall(phonemeSearchRegex, word)


"""
READ CORPUS FROM ASJP DUMP
"""
print("READ CORPUS FROM ASJP DUMP")
pathToASJPCorpusFile = "data/dataset.tab"
allWords = []
info_bagOfPhonemes = []
info_geo = []
language = []

for i,line in enumerate(codecs.open(pathToASJPCorpusFile,"r","utf-8")):
    if i > 0:
        line = line.split("\t")
        if "PROTO" not in line[0] and "ARTIFICIAL" not in line[2]:
            words = line[10:]
            #remove invalid characters
            for word in words:
                word = word.replace("%","")
            """
            for cells with more than one corresponding word, add that word as new entry
            """
            tba = []
            for i_w,word in enumerate(words):
                if "," in word:
                    for match in  regex.findall("(?<=,).+",word):          
                        tba.append(match)
                    #reduce entry to first occurence of seperator
                    words[i_w] = word[:word.index(",")]
            words.extend(tba)
            allWords.extend(words)

            #add language
            language.extend(line[0]*len(words))
            
            #add info_bagOfPhonemes
            #here its binary bag of phonemes 
            phonemes = set()
            for word in words:
                for phoneme in getListofASJPPhonemes(word):
                    phonemes.add(phoneme)
            
            info_bagOfPhonemes.append(phonemes)
            
            #add geo info
            info_geo.append([words[5],words[6]])

"""
EXTRACT ALL PHONEMES AND ADD WORD BOUNDARIES AND GET RID OF EMPTY STRINGS
"""
print("EXTRACT ALL PHONEMES AND ADD WORD BOUNDARIES AND GET RID OF EMPTY STRINGS")
allWords = [["<s>"]+getListofASJPPhonemes(word)+["</s>"] for word in allWords if len(word) > 0]

"""
CREATE ONE HOTS FOR LANGUAGES
"""
print("CREATE ONE HOTS FOR LANGUAGES")
allLanguages = set()
for l in language:
    allLanguages.add(l)
allLanguages = list(allLanguages)
lang_indices = dict((c, i) for i, c in enumerate(allLanguages))
indices_lang = dict((i, c) for i, c in enumerate(allLanguages))
for i,l in enumerate(language):
    language[i] =  np.zeros(len(allLanguages),dtype=np.bool)
    language[i][lang_indices[l]] = True

language = np.array(language) 

"""
CREATE BINARY BAG OF PHONEME FEATURES
"""
print("CREATE BINARY BAG OF PHONEME FEATURES")
allPhonemes = set()
for i in info_bagOfPhonemes:
    for p in i:
        allPhonemes.add(p)
allPhonemes  = list(allPhonemes,dtype=np.bool)
phoneme_indices = dict((c, i) for i, c in enumerate(allPhonemes))
indices_phoneme = dict((i, c) for i, c in enumerate(allPhonemes))
for i,inf in enumerate(info_bagOfPhonemes):
    info_bagOfPhonemes[i] = np.zeros(len(allPhonemes))
    for p in info_bagOfPhonemes:
        info_bagOfPhonemes[i][phoneme_indices[p]] = True
  
info_bagOfPhonemes = np.array(info_bagOfPhonemes)

info_geo = np.array(info_geo)

"""
CREATE 4-GRAMS
"""
maxlen = 15
data_train_phono_X = []
data_train_phono_y = []
data_train_info_bagOfPhonemes = []
data_train_info_geo = []
data_train_language = []
for i,word,lang,inf,geo in enumerate(allWords,language,info_bagOfPhonemes,info_geo):
    if len(word) >= 4:
        for ngram in ngrams(word,4):
            context = [phoneme_indices[phoneme] for phoneme in  ngram[:4]]
            #pad
            context += (maxlen-len(context))* [0]
            target = phoneme_indices[ngram[4]]
            data_train_phono_X.append(context)
            data_train_phono_y.append(target)
            data_train_info_bagOfPhonemes.append(inf)
            data_train_language.append(lang)
            data_train_info_geo.append(geo)
            
data_train_phono_X = np.array(data_train_phono_X)    
data_train_phono_y = np.array(data_train_phono_y)    
data_train_info_bagOfPhonemes = np.array(data_train_info_bagOfPhonemes)    
data_train_info_geo = np.array(data_train_info_geo)    
data_train_language = np.array(data_train_language)    

"""
MODEL
"""
n_symbols = 1000
dim_embedding_phono = 100
dim_embedding_word = 100
dim_embedding_language = 20
dim_embedding_info_bagOfPhonemes = 100
dim_embedding_info_geo = 100
n_languages  =100
n_info_bagOfPhonemes = 1000

#input phono
input_phono = Input((maxlen))
embedded_phoneme = Embedding(n_symbols, dim_embedding_phono, input_length=maxlen,mask_zero=True)(input_phono)
embedded_word = GRU(output_dim=dim_embedding_word, input_shape=(maxlen, len(n_symbols)))(embedded_phoneme)

#input language
input_language = Input((n_languages,))
embedded_language = Dense(dim_embedding_language,activation="linear")(input_language)

#input info_bagOfPhonemes
input_info_bagOfPhonemes = Input((1000,))
embedded_info_bagOfPhonemes = Dense(dim_embedding_info_bagOfPhonemes,activation="linear")(input_info_bagOfPhonemes)


#input info_geo
input_info_geo = Input((2,))
embedded_info_geo = Dense(dim_embedding_info_geo,activation="linear")(input_info_geo)

#concatenate everything
concat = Merge([embedded_word,embedded_language,embedded_info_bagOfPhonemes,embedded_info_geo],mode="concat")

#fully connected layer
concat = Dense(1000,activation="relu")(concat)

#classifier
y_predicted = Dense(n_symbols,activation="softmax")(concat)

model = Model([input_phono,input_language,input_info_bagOfPhonemes,input_info_geo],y_predicted)
model.compile("Adam","categorical_crossentropy",metrics=["accuracy"])

model.fit([data_train_phono_X,data_train_language,data_train_info_bagOfPhonemes,data_train_info_geo],data_train_phono_y,batch_size=100,nb_epoch=5)

phoneme_embedder = Model(input_phono,embedded_phoneme)
with codecs.open("phoneme_embeddings_plm.csv","w") as f:
    for phoneme in phoneme_indices:
        embedding = phoneme_embedder.predict(np.array([phoneme_indices[phoneme]]+(maxlen-1)*[0]))
        f.write("\""+phoneme+"\"")
        for emb in embedding:
            f.write(","+emb)
        f.write("\n")

language_embedder = Model(input_language,embedded_phoneme)
with codecs.open("lang_embeddings_plm.csv","w") as f:
    for lang in lang_indices:
        x = np.zeros((1,len(lang_indices)))
        embedding = phoneme_embedder.predict(np.array([lang_indices[phoneme]]+(maxlen-1)*[0]))
        f.write("\""+phoneme+"\"")
        for emb in embedding:
            f.write(","+emb)
        f.write("\n")


