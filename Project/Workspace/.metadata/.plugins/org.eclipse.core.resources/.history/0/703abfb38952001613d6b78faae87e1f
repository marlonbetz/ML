import numpy as np 
import regex  
import codecs
import sys
from scipy.spatial.distance import cosine
from sklearn.neighbors import KNeighborsClassifier
import evaluation

def vectorLinspace(start,stop,num=50):
    assert len(start) == len(stop)
    assert num > 0
    return np.array([np.linspace(start[dim],stop[dim],num) for dim in range(len(start))]).transpose()

def getListofASJPPhonemes(word):
    phonemes_alone="pbmfv84tdszcnSZCjT5kgxNqGX7hlLwyr!ieaouE3"
    phonemeSearchRegex = "["+phonemes_alone+"][\"\*]?(?!["+phonemes_alone+"]~|["+phonemes_alone+"]{2}\$)|["+phonemes_alone+"]{2}?~|["+phonemes_alone+"]{3}?\$"
    return regex.findall(phonemeSearchRegex, word)



"""
READ CORPUS FROM ASJP DUMP
"""
print("READ CORPUS FROM ASJP DUMP")
pathToASJPCorpusFile = "data/dataset.tab"
allWords = []
for i,line in enumerate(codecs.open(pathToASJPCorpusFile,"r","utf-8")):
    if i > 0:
        line = line.split("\t")
        if "PROTO" not in line[0] and "ARTIFICIAL" not in line[2]:
            words = line[10:]
            #remove invalid characters
            for word in words:
                word = word.replace("%","")
            """
            for cells with more than one corresponding word, add that word as new entry
            """
            tba = []
            for i_w,word in enumerate(words):
                if "," in word:
                    for match in  regex.findall("(?<=,).+",word):          
                        tba.append(match)
                    #reduce entry to first occurence of seperator
                    words[i_w] = word[:word.index(",")]
            words.extend(tba)
            allWords.extend(words)
   
"""
EXTRACT ALL PHONEMES AND ADD WORD BOUNDARIES AND GET RID OF EMPTY STRINGS
"""
print("EXTRACT ALL PHONEMES AND ADD WORD BOUNDARIES AND GET RID OF EMPTY STRINGS")
allWords = [["<s>"]+getListofASJPPhonemes(word)+["</s>"] for word in allWords if len(word) > 0]

"""
COUNT PHONEMES
"""
print("COUNT PHONEMES")
freq_phonemes = dict()
for i,word in enumerate(allWords):
    for phoneme in word:
        if phoneme not in freq_phonemes:
            freq_phonemes[phoneme] = 0
        freq_phonemes[phoneme] += 1
"""
REDUCE COMPLEX PHONEMES TO SINGLE PHONEMES IF FREQ SMALLER THAN X
"""

#hyperparameters to evaluate
minCountInterval =[1,5,10,15, 20,30,40,50 ]
dim_embeddingInterval = [2,5,10,20,50,
                         100,150
                         ,200,300,400,500,750,1000
                         ]
contextWindowInterval = [1,2,3,4,5,6,7,8,9,10]

n_gridSearchPoints = len(dim_embeddingInterval)
import matplotlib.pyplot as plt
cmap = plt.get_cmap('jet')
colors = cmap(np.linspace(0, 1, n_gridSearchPoints))
gridSearchPoint = 0
for dim_embedding in dim_embeddingInterval:
    accuracies = []
    losses = []
    for contextWindow  in contextWindowInterval:
        
        for minCount in minCountInterval:
            
            print("REDUCE COMPLEX PHONEMES TO SINGLE PHONEMES IF FREQ SMALLER THAN",minCount)
        
            for i,word in enumerate(allWords):
                tmp = []
                for phoneme in word:
                    if freq_phonemes[phoneme] >= minCount:
                        tmp.append(phoneme)
                    else:
                        tmp.extend(regex.findall("[pbmfv84tdszcnSZCjT5kgxNqGX7hlLwyr!ieaouE3]",phoneme))
                allWords[i] = tmp
            print(len(allWords))
            c = 0
            for word in allWords:
                c += len(word)
            print(c)
            from gensim.models import Word2Vec
            print("fitting model")
            w2v_model = Word2Vec(sentences=allWords,size=dim_embedding,min_count=1,window=contextWindow)
            """
            EVALUATION
            NOT YET IMPLEMENTED:
            If any phoneme in the evaluation set is not found directly in the training set due to rare occurrence,
            it is split up into its atomic phonemes which are then summed up.
            """
            print("EVALUATION")
            print(w2v_model.vocab.keys())
            c = 0
            sum_dist  =0
            for test in evaluation.SimilarityTestData:
                predicted = [s for (s,dist) in w2v_model.most_similar(positive=test["positive"], negative=test["negative"],topn=20)]
                true = test["true"][0]
                if true in predicted:
                    c+=1
                dist = cosine(w2v_model[predicted[0]],w2v_model[true])
                print("positive",test["positive"],
                      "negative",test["negative"],
                      "true",test["true"],
                      "predicted",w2v_model.most_similar(positive=test["positive"], negative=test["negative"],topn=1),
                      "distance to true label",dist)
                sum_dist += dist
            acc = c/len(evaluation.SimilarityTestData)
            loss = sum_dist/len(evaluation.SimilarityTestData)
            print("acc",acc)
            print("loss",loss)
            accuracies.append(acc)
            losses.append(loss)
    

    plt.plot(minCountInterval,accuracies,color=colors[gridSearchPoint],label="accuracy "+str(dim_embedding))
    plt.plot(minCountInterval,losses,color=colors[gridSearchPoint],label="loss " + str(dim_embedding))
    gridSearchPoint += 1
    

plt.legend()
plt.show()