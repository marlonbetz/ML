import numpy as np 
from keras.models import Model
from keras.layers import GRU,Dense,Input,Merge,Embedding
import regex
import codecs
def getListofASJPPhonemes(word):
    phonemes_alone="pbmfv84tdszcnSZCjT5kgxNqGX7hlLwyr!ieaouE3"
    phonemeSearchRegex = "["+phonemes_alone+"][\"\*]?(?!["+phonemes_alone+"]~|["+phonemes_alone+"]{2}\$)|["+phonemes_alone+"]{2}?~|["+phonemes_alone+"]{3}?\$"
    return regex.findall(phonemeSearchRegex, word)


"""
READ CORPUS FROM ASJP DUMP
"""
print("READ CORPUS FROM ASJP DUMP")
pathToASJPCorpusFile = "data/dataset.tab"
allWords = []
info = []
language = []

for i,line in enumerate(codecs.open(pathToASJPCorpusFile,"r","utf-8")):
    if i > 0:
        line = line.split("\t")
        if "PROTO" not in line[0] and "ARTIFICIAL" not in line[2]:
            words = line[10:]
            #remove invalid characters
            for word in words:
                word = word.replace("%","")
            """
            for cells with more than one corresponding word, add that word as new entry
            """
            tba = []
            for i_w,word in enumerate(words):
                if "," in word:
                    for match in  regex.findall("(?<=,).+",word):          
                        tba.append(match)
                    #reduce entry to first occurence of seperator
                    words[i_w] = word[:word.index(",")]
            words.extend(tba)
            allWords.extend(words)

            #add language
            language.extend(line[0]*len(words))
            
            #add info
            #here its binary bag of phonemes 
            phonemes = set()
            for word in words:
                for phoneme in getListofASJPPhonemes(word):
                    phonemes.add(phoneme)
            
            info.append(phonemes)

"""
EXTRACT ALL PHONEMES AND ADD WORD BOUNDARIES AND GET RID OF EMPTY STRINGS
"""
print("EXTRACT ALL PHONEMES AND ADD WORD BOUNDARIES AND GET RID OF EMPTY STRINGS")
allWords = [["<s>"]+getListofASJPPhonemes(word)+["</s>"] for word in allWords if len(word) > 0]

"""
CREATE ONE HOTS FOR LANGUAGES
"""
print("CREATE ONE HOTS FOR LANGUAGES")

"""
CREATE BINARY BAG OF PHONEME FEATURES
"""
print("CREATE BINARY BAG OF PHONEME FEATURES")
allPhonemes = set()
for i in info:
    for p in i:
        allPhonemes.add(p)
allPhonemes  = list(allPhonemes,dtype=np.bool)
for i,inf in enumerate(info):
    info[i] = np.zeros(len(allPhonemes))
    for p in info:
        info[i][allPhonemes.index(p)] = True
    



"""
MODEL
"""
maxlen = 10
n_symbols = 1000
dim_embedding_phono = 10
dim_embedding_word = 10
dim_embedding_language = 10
dim_embedding_info = 10
n_languages  =100
n_info = 1000

#input phono
input_phono = Input((maxlen,n_symbols))
embedded_phoneme = Embedding(n_symbols, dim_embedding_phono, input_length=maxlen)(input_phono)
embedded_word = GRU(output_dim=dim_embedding_word, input_shape=(maxlen, len(n_symbols)))(embedded_phoneme)

#input language
input_language = Input((n_languages,))
embedded_language = Dense(dim_embedding_language,activation="linear")(input_phono)

#input info
input_info = Input((1000,))
embedded_info = Dense(dim_embedding_info,activation="linear")(input_phono)

#concatenate everything
concat = Merge([embedded_word,embedded_language,embedded_info],mode="concat")

#fully connected layer
concat = Dense(1000,activation="relu")(concat)

#classifier
y_predicted = Dense(n_symbols,activation="softmax")(concat)

model = Model([input_phono,input_language,input_info],y_predicted)
model.compile("Adam","categorical_crossentropy")




