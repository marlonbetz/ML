\relax 
\citation{kim2015character,dos2014learning,zhang2015character}
\citation{mikolov2013efficient,mikolov2013distributed,pennington2014glove}
\citation{kiros2015skip}
\citation{kondrak2000new,rama2016siamese}
\citation{jager2014phylogenetic}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\citation{dos2014learning}
\citation{chen2015joint}
\citation{ling2015finding}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical and Practical Motivation}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Research}{3}}
\citation{bengio2003neural}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The neural language model as proposed in \cite  {bengio2003neural}.}}{4}}
\newlabel{fig:bengio2006network}{{1}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Embedding Models}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Count-based Embedding Models}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Latent Semantic Analysis}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}GloVe}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}SPPMI}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}SPPMI-SVD}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Neural Embedding Models}{4}}
\citation{collobert2008unified}
\citation{bengio2003neural}
\citation{bengio2003neural}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient,mikolov2013distributed}
\citation{bengio2003neural,collobert2008unified}
\newlabel{eq:bengio2006network_loss}{{1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Word2Vec}{5}}
\bibstyle{apa}
\bibdata{references}
\bibcite{bengio2003neural}{\astroncite {Bengio et\nobreakspace  {}al.}{2003}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The two common architectures of word2vec. The Continuous Bag-of-Words (CBOW) model predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. From \cite  {mikolov2013efficient}.}}{6}}
\newlabel{fig:word2vec}{{2}{6}}
\newlabel{eq:CBOW_loss}{{2}{6}}
\newlabel{eq:cbow_softmax_prob}{{3}{6}}
\newlabel{eq:skipgram_loss}{{4}{6}}
\newlabel{eq:skipgram_softmax_prob}{{5}{6}}
\bibcite{chen2015joint}{\astroncite {Chen et\nobreakspace  {}al.}{2015}}
\bibcite{collobert2008unified}{\astroncite {Collobert and Weston}{2008}}
\bibcite{dos2014learning}{\astroncite {dos Santos and Zadrozny}{2014}}
\bibcite{jager2014phylogenetic}{\astroncite {J{\"a}ger}{2014}}
\bibcite{kim2015character}{\astroncite {Kim et\nobreakspace  {}al.}{2015}}
\bibcite{kiros2015skip}{\astroncite {Kiros et\nobreakspace  {}al.}{2015}}
\bibcite{kondrak2000new}{\astroncite {Kondrak}{2000}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Evaluation Methods}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Results}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Use Cases}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Phonemic String Comparison}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Modeling Sound Change}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Phoneme Inventory Clustering}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Resume}{7}}
\bibcite{ling2015finding}{\astroncite {Ling et\nobreakspace  {}al.}{2015}}
\bibcite{mikolov2013efficient}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013a}}
\bibcite{mikolov2013distributed}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013b}}
\bibcite{pennington2014glove}{\astroncite {Pennington et\nobreakspace  {}al.}{2014}}
\bibcite{rama2016siamese}{\astroncite {Rama}{2016}}
\bibcite{zhang2015character}{\astroncite {Zhang et\nobreakspace  {}al.}{2015}}
