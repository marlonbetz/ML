\relax 
\citation{kim2015character,dos2014learning,zhang2015character}
\citation{mikolov2013efficient,mikolov2013distributed,pennington2014glove}
\citation{kiros2015skip}
\citation{kondrak2000new,rama2016siamese}
\citation{jager2014phylogenetic}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\citation{Goodfellow-et-al-2016-Book}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical and Practical Motivation}{3}}
\citation{dos2014learning}
\citation{chen2015joint}
\citation{ling2015finding}
\citation{bengio2003neural}
\citation{bengio2003neural}
\citation{landauer2013handbook,pennington2014glove}
\citation{bengio2003neural}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Research}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Embedding Models}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Neural Embedding Models}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Neural Language Models}{4}}
\citation{collobert2008unified}
\citation{bengio2003neural}
\citation{bengio2003neural}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The neural language model as proposed in \cite  {bengio2003neural}.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:bengio2006network}{{1}{5}}
\newlabel{eq:bengio2006network_loss}{{1}{5}}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient,mikolov2013distributed}
\citation{bengio2003neural,collobert2008unified}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Word2Vec}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The two common architectures of word2vec. The Continuous Bag-of-Words (CBOW) model predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. From \cite  {mikolov2013efficient}.\relax }}{6}}
\newlabel{fig:word2vec}{{2}{6}}
\citation{morin2005hierarchical}
\citation{chen2015strategies}
\citation{chen2015strategies}
\newlabel{eq:CBOW_loss}{{2}{7}}
\newlabel{eq:cbow_softmax_prob}{{3}{7}}
\newlabel{eq:skipgram_loss}{{4}{7}}
\newlabel{eq:skipgram_softmax_prob}{{5}{7}}
\newlabel{eq:hierarchicalsoftmax_nodeprob}{{6}{7}}
\citation{liu2008monte,bengio2003quick,bengio2008adaptive,cho2015using}
\citation{gutmann2010noise,mnih2012fast}
\newlabel{eq:negative_reinforcement_1}{{7}{8}}
\newlabel{eq:negative_reinforcement_2}{{8}{8}}
\newlabel{eq:negative_reinforcement_3}{{9}{8}}
\newlabel{eq:negative_reinforcement_4}{{10}{8}}
\newlabel{eq:negative_reinforcement_5}{{11}{8}}
\newlabel{eq:negative_reinforcement_6}{{12}{8}}
\newlabel{eq:negative_reinforcement_7}{{13}{8}}
\newlabel{eq:final_negative_reinforcement}{{14}{8}}
\citation{vaswani2013decoding,mnih2012fast}
\newlabel{eq:nce_1}{{15}{9}}
\newlabel{eq:nce_2}{{16}{9}}
\newlabel{eq:nce_3}{{17}{9}}
\newlabel{eq:nce_4}{{18}{9}}
\newlabel{eq:nce_5}{{19}{9}}
\citation{goldberg2014word2vec}
\citation{tsvetkov2016polyglot}
\citation{tsvetkov2016polyglot}
\citation{hochreiter1997long}
\citation{chen2015joint,ling2015finding,kim2015character,dos2014learning}
\citation{zhang2015character}
\citation{tsvetkov2016polyglot}
\citation{wals}
\citation{phoible}
\citation{lewis2015ethnologue}
\newlabel{eq:nce_6}{{20}{10}}
\newlabel{eq:nce_6}{{21}{10}}
\newlabel{eq:nce_7}{{22}{10}}
\newlabel{eq:neg_sampling_1}{{23}{10}}
\newlabel{eq:neg_sampling_2}{{24}{10}}
\newlabel{eq:neg_sampling_3}{{25}{10}}
\newlabel{eq:neg_sampling_3}{{26}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The Polyglot Language Model. From \cite  {tsvetkov2016polyglot}\relax }}{11}}
\newlabel{fig:plm}{{3}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Polyglot Neural Language Models}{12}}
\newlabel{eq:plm_1}{{27}{12}}
\newlabel{eq:plm_2}{{28}{12}}
\newlabel{eq:plm_3}{{29}{12}}
\newlabel{eq:plm_4}{{30}{12}}
\newlabel{eq:plm_5}{{31}{12}}
\newlabel{eq:plm_6}{{32}{12}}
\citation{wichmann2010asjp}
\citation{schnabel2015evaluation}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Intrinsic Evaluation}{13}}
\newlabel{Intrinsic Evaluation}{{5.2}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces t-SNE visualizations of the embeddings created by word2vec. (top left) The model learns to clearly separate natural classes such as vowels, plain pulmonic or glottalized consonants, while other articulations seem to spread over the feature space. (top right) A more detailed view on plain pulmonic consonants. Note the linear dependencies between voiced and unvoiced plosives and their respective nasal variant. (bottom left) Another detailed view. Note how the labialized uvular sounds cluster among glottalized consonants. (top right) The model seems to capture different manners of articulations across articulation type boundaries, as the linear dependency shows here.\relax }}{14}}
\newlabel{fig:word2vec_all}{{4}{14}}
\citation{rama2016siamese,jager2014phylogenetic}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces t-SNE visualizations of the embeddings created by the PLM. (left) Vowels as well as glottalizes sounds cluster among each other, while other classes show big variance over the embedding space. (middle) A more detailed view. Nasalized vowel still cluster closer to glottalized sounds. (right) Embeddings for some Germanic and Slavic languages. The Model does not seem to capture the relations between the languages too well.\relax }}{15}}
\newlabel{fig:plm_all}{{5}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of the four word2vec models evaluated. (top left) Both CBOW Models perform better than the skip-gram models over all numbers of embedding dimension. (bottom left) Mean distance between the predicted vector and the target with regard to embedding dimensions. Here, negative sampling yields less error than hierarchical softmax. (top right) Mean accuracy for the models with regard to the context window size. The models perform worse the bigger the context is. (bottom right) Mean distance between the predicted vector and the target with regard to context window size. Again, bigger contexts lead to worse predictions. \relax }}{16}}
\newlabel{fig:4models_comparison_meanAccLossTopn1}{{6}{16}}
\citation{tsvetkov2016polyglot}
\citation{wichmann2011phonological}
\citation{chung2014empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Mean accuracies of the four models tested, over the number of embedding dimensions (left) and the context window size (right)\relax }}{17}}
\newlabel{fig:4models_comparison_topn1_heatmap}{{7}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Mean and maximal accuracies of the skip-gram model with regard to the window size and number of negative samples over all other parameters.\relax }}{18}}
\newlabel{fig:acc_k_neg_vs_windowSize}{{8}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Discussion}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Mean accuracies for analogies with an analogy threshold of 1. The baseline model does not seem to be able to capture the compositionality of the latent features too well. (top left) Accuracy is best for plosives across plain pulmonic as well as more complex articulations, while fricatives perform worse. (top right) Glottalized and Plain pulmonic consonant phonemes yield best performance. Among complex articulations, which all perform bad, labialized phonemes yield best results, while aspirated and palatalized phonemes perform even worse. (bottom left) Adding voice works better than removing it or adding nasality. (bottom right) Vowel analogies work far better than consonant analogies. This should be due to the small number of possible vowel phonemes.\relax }}{19}}
\newlabel{fig:detailed_evaluation_topn1}{{9}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Mean accuracies for consonant analogies with an analogy threshold of 20. Here, the model is expected to yield better results, as wrong predictions are not as penalizing. (left) Accuracy is reasonable for plosives across plain pulmonic as well as more complex articulations. For fricatives, the accuracy is higher, but fricatives were only evaluated for plain pulmonic articulation. (middle) Plain pulmonic consonant phonemes nearly yield optimal accuracy, while glottalized phonemes still yield good results. Among complex articulations, labialized phonemes yield best results, while aspirated and palatalized phonemes perform far under average. (right) Removing voice yields better results than adding it. Applying nasality to plosives even performs better.\relax }}{20}}
\newlabel{fig:detailed_evaluation_topn20}{{10}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Mean accuracies for analogies with an analogy threshold of 1 on the pooled consonant phonemes. Here, the model is also expected to yield better results, the number of latent features should be reduced. (left) Accuracy is doubled compared with the unpooled phonemes. (middle) Adding voice works quite well, while removing it still yields acceptable performance. Applying nasality again works quite bad. (right) Vowel analogy tasks seem to work reasonably well, but worse than with unpooled consonants.\relax }}{20}}
\newlabel{fig:detailed_evaluation_topn1_poooled}{{11}{20}}
\bibstyle{apa}
\bibdata{references}
\bibcite{Goodfellow-et-al-2016-Book}{\astroncite {Bengio and Courville}{2016}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Frequency plotted against mean accuracy in log space. It is clearly visible how the bad performance of the complex phonemes is due to their underrepresentation in the corpus.\relax }}{21}}
\newlabel{fig:freq_acc_correlation_logspace_topn1}{{12}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Resume}{21}}
\bibcite{bengio2003neural}{\astroncite {Bengio et\nobreakspace  {}al.}{2003a}}
\bibcite{bengio2008adaptive}{\astroncite {Bengio and Sen{\'e}cal}{2008}}
\bibcite{bengio2003quick}{\astroncite {Bengio et\nobreakspace  {}al.}{2003b}}
\bibcite{chen2015strategies}{\astroncite {Chen et\nobreakspace  {}al.}{2015a}}
\bibcite{chen2015joint}{\astroncite {Chen et\nobreakspace  {}al.}{2015b}}
\bibcite{cho2015using}{\astroncite {Cho et\nobreakspace  {}al.}{2015}}
\bibcite{chung2014empirical}{\astroncite {Chung et\nobreakspace  {}al.}{2014}}
\bibcite{collobert2008unified}{\astroncite {Collobert and Weston}{2008}}
\bibcite{dos2014learning}{\astroncite {dos Santos and Zadrozny}{2014}}
\bibcite{wals}{\astroncite {Dryer and Haspelmath}{2013}}
\bibcite{goldberg2014word2vec}{\astroncite {Goldberg and Levy}{2014}}
\bibcite{gutmann2010noise}{\astroncite {Gutmann and Hyv{\"a}rinen}{2010}}
\bibcite{hochreiter1997long}{\astroncite {Hochreiter and Schmidhuber}{1997}}
\bibcite{jager2014phylogenetic}{\astroncite {J{\"a}ger}{2014}}
\bibcite{kim2015character}{\astroncite {Kim et\nobreakspace  {}al.}{2015}}
\bibcite{kiros2015skip}{\astroncite {Kiros et\nobreakspace  {}al.}{2015}}
\bibcite{kondrak2000new}{\astroncite {Kondrak}{2000}}
\bibcite{landauer2013handbook}{\astroncite {Landauer et\nobreakspace  {}al.}{2013}}
\bibcite{lewis2015ethnologue}{\astroncite {Lewis et\nobreakspace  {}al.}{2015}}
\bibcite{ling2015finding}{\astroncite {Ling et\nobreakspace  {}al.}{2015}}
\bibcite{liu2008monte}{\astroncite {Liu}{2008}}
\bibcite{mikolov2013efficient}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013a}}
\bibcite{mikolov2013distributed}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013b}}
\bibcite{mnih2012fast}{\astroncite {Mnih and Teh}{2012}}
\bibcite{phoible}{\astroncite {Moran et\nobreakspace  {}al.}{2014}}
\bibcite{morin2005hierarchical}{\astroncite {Morin and Bengio}{2005}}
\bibcite{pennington2014glove}{\astroncite {Pennington et\nobreakspace  {}al.}{2014}}
\bibcite{rama2016siamese}{\astroncite {Rama}{2016}}
\bibcite{schnabel2015evaluation}{\astroncite {Schnabel et\nobreakspace  {}al.}{2015}}
\bibcite{tsvetkov2016polyglot}{\astroncite {Tsvetkov et\nobreakspace  {}al.}{2016}}
\bibcite{vaswani2013decoding}{\astroncite {Vaswani et\nobreakspace  {}al.}{2013}}
\bibcite{wichmann2010asjp}{\astroncite {Wichmann et\nobreakspace  {}al.}{2010}}
\bibcite{wichmann2011phonological}{\astroncite {Wichmann et\nobreakspace  {}al.}{2011}}
\bibcite{zhang2015character}{\astroncite {Zhang et\nobreakspace  {}al.}{2015}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix}{24}}
\citation{jager2014phylogenetic}
\citation{jager2014phylogenetic}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Consonant analogy tests. All analogy tasks consist of 2 positive values, one negative value and a target value, as described in 5.2\hbox {}.\relax }}{25}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Consonant analogy tests (continued)\relax }}{26}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Vowel analogy tests\relax }}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces ASJP consonants with their IPA equivalents. Glottalization is marked by a following double quote, coarticulation is indicated by a tilde ∼ (if one co-articulator) or a dollar sign \$ (if two co-articulators). The tables follow \cite  {jager2014phylogenetic}.\relax }}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces ASJP vowels with their IPA equivalents. Nasality is marked by a following asterisk.\relax }}{29}}
